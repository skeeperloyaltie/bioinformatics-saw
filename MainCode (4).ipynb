{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f7df1",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing for RNA Sequences\n",
    "\n",
    "### Description:\n",
    "### Load Data Function (load_data):\n",
    "- This function takes a file path and a column name as inputs. It reads the data from the specified CSV file into a dataframe (a table-like structure in Python).\n",
    "- It checks if there are any missing values in the specified column that contains RNA sequences. If missing values are found, it removes those rows from the dataframe to ensure that the analysis is done only on complete data.\n",
    "### One-Hot Encoding Function (one_hot_encode):\n",
    "- This function takes an RNA sequence as input. RNA sequences are made up of nucleotides represented by the letters A, C, G, and U.\n",
    "- It checks if the input sequence is in the correct format (i.e., it's a string). If itâ€™s not, the function returns None, which is a way of indicating missing or incorrect data.\n",
    "- The sequence is then converted to uppercase and any occurrence of 'T' (thymine, which is found in DNA but not RNA) is replaced with 'U' (uracil, which is specific to RNA).\n",
    "- The function then converts the sequence into a one-hot encoded format. This means each nucleotide (A, C, G, U) is represented as a vector (a list of numbers). For example, \n",
    "A is represented as [1, 0, 0, 0], C as [0, 1, 0, 0], G as [0, 0, 1, 0], and U as [0, 0, 0, 1]. This numerical representation is useful for computational models that require numeric input.\n",
    "### Loading Specific Datasets (ENCORI and LncBase):\n",
    "- The code then loads two datasets from CSV files: one called ENCORI_miRNA_lncRNA.csv and another called lncbase_with_sequences.csv. These datasets contain RNA sequences along with other biological information.\n",
    "It applies the one-hot encoding function to specific columns in these datasets that contain RNA sequences. This prepares the data for further analysis or modeling, where understanding the relationships between sequences can be crucial, such as predicting RNA interactions or functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62aa168-1030-480d-91ba-a702cc0ea90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_encode_data(file_path, sequence_column, structure_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(subset=[sequence_column, structure_column], inplace=True)\n",
    "    df['encoded_sequence'] = df[sequence_column].apply(one_hot_encode)\n",
    "    df['encoded_structure'] = df[structure_column].apply(encode_structure)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'U': [0, 0, 0, 1]}\n",
    "    return np.array([mapping.get(nucleotide, [0, 0, 0, 0]) for nucleotide in sequence.upper().replace('T', 'U')], dtype=np.float32)\n",
    "\n",
    "def encode_structure(structure):\n",
    "    mapping = {'.': [1, 0, 0], '(': [0, 1, 0], ')': [0, 0, 1]}\n",
    "    return np.array([mapping.get(char, [0, 0, 0]) for char in structure], dtype=np.float32)\n",
    "\n",
    "def pad_encoded_data(encoded_data, max_length):\n",
    "    padded = np.zeros((len(encoded_data), max_length, len(encoded_data[0][0])), dtype=np.float32)\n",
    "    for i, seq in enumerate(encoded_data):\n",
    "        length = min(len(seq), max_length)\n",
    "        padded[i, :length, :] = seq[:length]\n",
    "    return padded\n",
    "\n",
    "def create_overlapping_windows(data, window_size, overlap):\n",
    "    step = window_size - overlap\n",
    "    num_windows = max(1, (len(data) - window_size) // step + 1)\n",
    "    windows = [data[i * step: i * step + window_size] for i in range(num_windows) if i * step + window_size <= len(data)]\n",
    "    return np.array(windows)\n",
    "\n",
    "def prepare_dataset(file_path, sequence_column, structure_column, min_window_size=50, max_window_size=500, overlap_ratio=0.5, label_column=None, threshold=None):\n",
    "    df = load_and_encode_data(file_path, sequence_column, structure_column)\n",
    "    \n",
    "    # Determine appropriate window size based on the shortest sequence\n",
    "    shortest_seq_length = df['encoded_sequence'].apply(len).min()\n",
    "    window_size = min(max_window_size, max(min_window_size, shortest_seq_length))\n",
    "    overlap = int(window_size * overlap_ratio)\n",
    "    \n",
    "    df['padded_sequences'] = df['encoded_sequence'].apply(lambda x: pad_encoded_data([x], window_size)[0])\n",
    "    df['padded_structures'] = df['encoded_structure'].apply(lambda x: pad_encoded_data([x], window_size)[0])\n",
    "    \n",
    "    df['sequence_windows'] = df['padded_sequences'].apply(lambda x: create_overlapping_windows(x, window_size, overlap))\n",
    "    df['structure_windows'] = df['padded_structures'].apply(lambda x: create_overlapping_windows(x, window_size, overlap))\n",
    "    \n",
    "    df['integrated_data'] = df.apply(lambda row: np.concatenate((row['sequence_windows'], row['structure_windows']), axis=-1), axis=1)\n",
    "\n",
    "    if label_column:\n",
    "        df['labels'] = df[label_column].apply(lambda x: 1 if (threshold is not None and x >= threshold) else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2af203-3779-4c89-8c8f-8caec5199525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e919ce69-d299-4e38-ad21-975ab29f27d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/lncbase_with_sequences.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_333132/1937263296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m lncrna_dataset = prepare_dataset(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m'dataset/lncbase_with_sequences.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sequence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sequence_structure'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabel_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'positive_negative'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_333132/2299315601.py\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(file_path, sequence_column, structure_column, min_window_size, max_window_size, overlap_ratio, label_column, threshold)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_window_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_window_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_encode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Determine appropriate window size based on the shortest sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_333132/2299315601.py\u001b[0m in \u001b[0;36mload_and_encode_data\u001b[0;34m(file_path, sequence_column, structure_column)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_encode_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoded_sequence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/lncbase_with_sequences.csv'"
     ]
    }
   ],
   "source": [
    "mirna_dataset = prepare_dataset(\n",
    "    'dataset/mirna_sequences.csv', 'miRseq', 'miRseq_structure', \n",
    "    label_column='clipExpNum', threshold=10\n",
    ")\n",
    "\n",
    "lncrna_dataset = prepare_dataset(\n",
    "    'dataset/lncbase_with_sequences.csv', 'Sequence', 'Sequence_structure', \n",
    "    label_column='positive_negative'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344197f-24e3-4ae4-af9e-6dc98e81997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd105532-09d8-419b-9794-f676384943d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncrna_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9835744",
   "metadata": {},
   "source": [
    "## Mirna Dataset Sequence  transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae54c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 23:22:11.659235: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-05 23:22:11.659298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-05 23:22:11.660289: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-05 23:22:11.665588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 23:22:12.626440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_positional_encoding(seq_length, model_size):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(model_size) // 2)) / np.float32(model_size))\n",
    "    angle_rads = np.arange(seq_length)[:, np.newaxis] * angle_rates\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = np.expand_dims(pos_encoding, 0)  # Add batch dimension for broadcasting\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(inputs, model_size, num_heads, ff_dim, dropout_rate):\n",
    "    seq_length = inputs.shape[1]  # Ensure this is correct\n",
    "    model_size = inputs.shape[2]  # Ensure this matches the last dimension of inputs\n",
    "    pos_encoding = get_positional_encoding(seq_length, model_size)\n",
    "    inputs += pos_encoding  # Add positional encoding to inputs\n",
    "\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_size, dropout=dropout_rate)\n",
    "    attention_output = attention_layer(inputs, inputs)\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = tf.keras.layers.Add()([inputs, attention_output])\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "\n",
    "    ff_layer_one = tf.keras.layers.Dense(ff_dim, activation='relu')\n",
    "    ff_layer_two = tf.keras.layers.Dense(model_size)\n",
    "    ff_output = ff_layer_one(attention_output)\n",
    "    ff_output = tf.keras.layers.Dropout(dropout_rate)(ff_output)\n",
    "    ff_output = ff_layer_two(ff_output)\n",
    "    ff_output = tf.keras.layers.Add()([attention_output, ff_output])\n",
    "    ff_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ff_output)\n",
    "\n",
    "    return ff_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ca4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_sequence_only_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Applying multiple layers of the transformer encoder\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # Removing any singleton dimensions and applying pooling\n",
    "    if x.shape[1] == 1:\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "X = np.array(mirna_dataset['sequence_windows'].tolist())\n",
    "y = np.array(mirna_dataset['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d142bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after processing: (424, 100, 4)\n",
      "Shape of y: (424,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 23:22:15.111302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38375 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-05-05 23:22:15.113640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38375 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "2024-05-05 23:22:15.115822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38375 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:e2:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 23:22:19.778144: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-05 23:22:20.984309: I external/local_xla/xla/service/service.cc:168] XLA service 0x151aa55d5690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-05 23:22:20.984352: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-05-05 23:22:20.984358: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-05-05 23:22:20.984362: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-05-05 23:22:20.988751: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-05 23:22:21.017430: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714969341.092154  333433 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 10s 62ms/step - loss: 0.7124 - accuracy: 0.4985 - val_loss: 0.6770 - val_accuracy: 0.6471\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.6966 - accuracy: 0.4956 - val_loss: 0.6805 - val_accuracy: 0.6588\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6929 - accuracy: 0.5133 - val_loss: 0.7139 - val_accuracy: 0.3529\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6949 - accuracy: 0.5044 - val_loss: 0.7026 - val_accuracy: 0.3529\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6939 - accuracy: 0.5015 - val_loss: 0.6837 - val_accuracy: 0.6588\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6922 - accuracy: 0.5369 - val_loss: 0.6946 - val_accuracy: 0.4353\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6928 - accuracy: 0.4985 - val_loss: 0.6987 - val_accuracy: 0.3882\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6969 - accuracy: 0.4690 - val_loss: 0.6821 - val_accuracy: 0.6588\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6920 - accuracy: 0.5192 - val_loss: 0.7090 - val_accuracy: 0.3529\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6923 - accuracy: 0.5103 - val_loss: 0.6966 - val_accuracy: 0.3882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulating the loading and processing of sequence data\n",
    "def simulate_data_processing():\n",
    "    # Simulate some sequence data\n",
    "    sequence_data = np.random.randint(0, 4, (424, 100))  # Example: 424 sequences of length 100\n",
    "    # One-hot encode the sequence data\n",
    "    sequence_encoded = np.eye(4)[sequence_data]  # Example: one-hot encoding\n",
    "    return sequence_encoded.reshape(424, -1, 4)  # Reshape for model input: (batch, sequence_length, features)\n",
    "\n",
    "# Load and process data\n",
    "X = simulate_data_processing()\n",
    "y = np.random.randint(0, 2, 424)  # Random binary labels\n",
    "\n",
    "print(\"Shape of X after processing:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = X.shape[1:]  # Dynamic input shape based on data\n",
    "num_layers = 4\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build the model, ensuring the input shape is correctly passed\n",
    "try:\n",
    "    sequence_model = build_sequence_only_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout)\n",
    "except Exception as e:\n",
    "    print(\"Error in building the model:\", e)\n",
    "    raise\n",
    "\n",
    "# Split data for training\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "except Exception as e:\n",
    "    print(\"Error during train-test split:\", e)\n",
    "    raise\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = sequence_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "except Exception as e:\n",
    "    print(\"Error during model training:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31caf7",
   "metadata": {},
   "source": [
    "## miRNA Structure train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334367c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(mirna_dataset['structure_windows'].tolist())\n",
    "y = np.array(mirna_dataset['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faeb33d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after processing: (424, 100, 4)\n",
      "Shape of y: (424,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 8s 56ms/step - loss: 0.7071 - accuracy: 0.5251 - val_loss: 0.6920 - val_accuracy: 0.5294\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6945 - accuracy: 0.4631 - val_loss: 0.6911 - val_accuracy: 0.5647\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6935 - accuracy: 0.5015 - val_loss: 0.6868 - val_accuracy: 0.5529\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6939 - accuracy: 0.5251 - val_loss: 0.6877 - val_accuracy: 0.5529\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6931 - accuracy: 0.5251 - val_loss: 0.6870 - val_accuracy: 0.5529\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6940 - accuracy: 0.5133 - val_loss: 0.6872 - val_accuracy: 0.5529\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6930 - accuracy: 0.5251 - val_loss: 0.6873 - val_accuracy: 0.5529\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6937 - accuracy: 0.5133 - val_loss: 0.6891 - val_accuracy: 0.5882\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6944 - accuracy: 0.5251 - val_loss: 0.6858 - val_accuracy: 0.5529\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.6935 - accuracy: 0.5251 - val_loss: 0.6885 - val_accuracy: 0.5647\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulating the loading and processing of sequence data\n",
    "def simulate_data_processing():\n",
    "    # Simulate some sequence data\n",
    "    sequence_data = np.random.randint(0, 4, (424, 100))  # Example: 424 sequences of length 100\n",
    "    # One-hot encode the sequence data\n",
    "    sequence_encoded = np.eye(4)[sequence_data]  # Example: one-hot encoding\n",
    "    return sequence_encoded.reshape(424, -1, 4)  # Reshape for model input: (batch, sequence_length, features)\n",
    "\n",
    "# Load and process data\n",
    "X = simulate_data_processing()\n",
    "y = np.random.randint(0, 2, 424)  # Random binary labels\n",
    "\n",
    "print(\"Shape of X after processing:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = X.shape[1:]  # Dynamic input shape based on data\n",
    "num_layers = 4\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build the model, ensuring the input shape is correctly passed\n",
    "try:\n",
    "    sequence_model = build_sequence_only_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout)\n",
    "except Exception as e:\n",
    "    print(\"Error in building the model:\", e)\n",
    "    raise\n",
    "\n",
    "# Split data for training\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "except Exception as e:\n",
    "    print(\"Error during train-test split:\", e)\n",
    "    raise\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = sequence_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "except Exception as e:\n",
    "    print(\"Error during model training:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9de14",
   "metadata": {},
   "source": [
    "# miRNA Structure + Sequence Transformer model - intergrated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f04e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(mirna_dataset['integrated_data'].tolist())\n",
    "y = np.array(mirna_dataset['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b781decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after processing: (424, 100, 4)\n",
      "Shape of y: (424,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 8s 56ms/step - loss: 0.6947 - accuracy: 0.5044 - val_loss: 0.6903 - val_accuracy: 0.5529\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.6931 - accuracy: 0.5280 - val_loss: 0.6875 - val_accuracy: 0.5529\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.6931 - accuracy: 0.5280 - val_loss: 0.6875 - val_accuracy: 0.5529\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.6921 - accuracy: 0.5280 - val_loss: 0.6894 - val_accuracy: 0.5529\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6923 - accuracy: 0.5280 - val_loss: 0.6881 - val_accuracy: 0.5529\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6918 - accuracy: 0.5280 - val_loss: 0.6879 - val_accuracy: 0.5529\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6916 - accuracy: 0.5280 - val_loss: 0.6879 - val_accuracy: 0.5529\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6917 - accuracy: 0.5280 - val_loss: 0.6880 - val_accuracy: 0.5529\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.6915 - accuracy: 0.5280 - val_loss: 0.6882 - val_accuracy: 0.5529\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.6919 - accuracy: 0.5280 - val_loss: 0.6872 - val_accuracy: 0.5529\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulating the loading and processing of sequence data\n",
    "def simulate_data_processing():\n",
    "    # Simulate some sequence data\n",
    "    sequence_data = np.random.randint(0, 4, (424, 100))  # Example: 424 sequences of length 100\n",
    "    # One-hot encode the sequence data\n",
    "    sequence_encoded = np.eye(4)[sequence_data]  # Example: one-hot encoding\n",
    "    return sequence_encoded.reshape(424, -1, 4)  # Reshape for model input: (batch, sequence_length, features)\n",
    "\n",
    "# Load and process data\n",
    "X = simulate_data_processing()\n",
    "y = np.random.randint(0, 2, 424)  # Random binary labels\n",
    "\n",
    "print(\"Shape of X after processing:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = X.shape[1:]  # Dynamic input shape based on data\n",
    "num_layers = 4\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build the model, ensuring the input shape is correctly passed\n",
    "try:\n",
    "    sequence_model = build_sequence_only_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout)\n",
    "except Exception as e:\n",
    "    print(\"Error in building the model:\", e)\n",
    "    raise\n",
    "\n",
    "# Split data for training\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "except Exception as e:\n",
    "    print(\"Error during train-test split:\", e)\n",
    "    raise\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = sequence_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "except Exception as e:\n",
    "    print(\"Error during model training:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ab7f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.6872 - accuracy: 0.5529 - 56ms/epoch - 19ms/step\n",
      "Test accuracy: 0.5529412031173706, Test loss: 0.6871737241744995\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = sequence_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}, Test loss: {test_loss}\")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44495193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fbc31fd",
   "metadata": {},
   "source": [
    "## lncRNA Dataset Transfomer - For structure, sequence, structure + sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c97b7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, num_heads, model_size, ff_dim, dropout_rate):\n",
    "    # Multi-head self-attention\n",
    "    attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_size)(inputs, inputs)\n",
    "    attention = tf.keras.layers.Dropout(dropout_rate)(attention)\n",
    "    attention_out = tf.keras.layers.Add()([inputs, attention])  # Skip connection\n",
    "    attention_out = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_out)\n",
    "\n",
    "    # Feed-forward network\n",
    "    ff_net = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(ff_dim, activation='relu'),  # First dense layer\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(model_size)  # Ensures output dimension matches model_size\n",
    "    ])\n",
    "    ff_out = ff_net(attention_out)\n",
    "    ff_out = tf.keras.layers.Add()([attention_out, ff_out])  # Skip connection\n",
    "    ff_out = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ff_out)\n",
    "\n",
    "    return ff_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1061c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(input_shape, num_layers, num_heads, model_size, ff_dim, dropout):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, num_heads, model_size, ff_dim, dropout)\n",
    "    \n",
    "    # Output layer: Adjust according to the problem (classification or regression)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a2f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5671 entries, 0 to 5670\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   geneId              5671 non-null   object\n",
      " 1   geneName            5671 non-null   object\n",
      " 2   mirna               5671 non-null   object\n",
      " 3   species             5671 non-null   object\n",
      " 4   cell_line           4303 non-null   object\n",
      " 5   tissue              5428 non-null   object\n",
      " 6   category            5671 non-null   object\n",
      " 7   method              5671 non-null   object\n",
      " 8   positive_negative   5671 non-null   object\n",
      " 9   direct_indirect     5671 non-null   object\n",
      " 10  condition           2614 non-null   object\n",
      " 11  Sequence            5671 non-null   object\n",
      " 12  Sequence_structure  5671 non-null   object\n",
      " 13  encoded_sequence    5671 non-null   object\n",
      " 14  encoded_structure   5671 non-null   object\n",
      " 15  padded_sequences    5671 non-null   object\n",
      " 16  padded_structures   5671 non-null   object\n",
      " 17  sequence_windows    5671 non-null   object\n",
      " 18  structure_windows   5671 non-null   object\n",
      " 19  integrated_data     5671 non-null   object\n",
      " 20  labels              5671 non-null   int64 \n",
      "dtypes: int64(1), object(20)\n",
      "memory usage: 974.7+ KB\n"
     ]
    }
   ],
   "source": [
    "lncrna_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4417a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5671\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the label distribution\n",
    "label_counts = lncrna_dataset['labels'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f91394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in ./.local/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.9/site-packages (from imbalanced-learn) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.local/lib/python3.9/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/apps/gcc11_2/python3/3.9.7/lib/python3.9/site-packages/scipy-1.6.1-py3.9-linux-x86_64.egg (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./.local/lib/python3.9/site-packages (from imbalanced-learn) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.9/site-packages (from imbalanced-learn) (1.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/opt/apps/gcc11_2/python3/3.9.7/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The target 'y' needs to have more than 1 class. Got 1 class instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_301850/959712200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Applying SMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Check the new label distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;34mf\"The target 'y' needs to have more than 1 class. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;34mf\"Got {np.unique(y).size} class instead\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The target 'y' needs to have more than 1 class. Got 1 class instead"
     ]
    }
   ],
   "source": [
    "# Example initialization\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "input_shape = (1, 59, 7)  # As per your data\n",
    "model = build_transformer_model(input_shape=(59, 7), num_layers=4, num_heads=8, model_size=7, ff_dim=256, dropout=0.1)\n",
    "\n",
    "# Training\n",
    "X = lncrna_dataset['integrated_data']\n",
    "y = lncrna_dataset['labels']\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'X' is a DataFrame where each row might contain nested arrays\n",
    "# def flatten_data(X):\n",
    "#     # Flatten each entry if it's an array; otherwise, keep it as is\n",
    "#     flattened_X = np.array([x.flatten() if isinstance(x, np.ndarray) else x for x in X])\n",
    "#     return flattened_X\n",
    "\n",
    "# # Example of flattening the data\n",
    "# X_flattened = flatten_data(X.values)  # Make sure 'X' is suitable for this operation\n",
    "\n",
    "# # Now, 'X_flattened' can be fed into train_test_split or SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# # Splitting the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_flattened, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Applying SMOTE\n",
    "# smote = SMOTE()\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Check the new label distribution\n",
    "# print(np.unique(y_resampled, return_counts=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def convert_if_needed(array):\n",
    "    # Ensure array is a numpy array for consistent handling\n",
    "    if isinstance(array, pd.DataFrame) or isinstance(array, pd.Series):\n",
    "        array = array.to_numpy()\n",
    "    \n",
    "    # Check if the first element is an array (use .iloc for DataFrame/Series safety)\n",
    "    if isinstance(array[0], np.ndarray):\n",
    "        array = np.vstack(array)  # This might be necessary if elements are arrays\n",
    "    \n",
    "    return tf.convert_to_tensor(array, dtype=tf.float32)  # Convert to tensor\n",
    "\n",
    "# Assuming 'X' and 'y' have been defined and are Pandas DataFrame/Series\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert data to tensors if not already\n",
    "X_train = convert_if_needed(X_train)\n",
    "X_test = convert_if_needed(X_test)\n",
    "y_train = convert_if_needed(y_train)\n",
    "y_test = convert_if_needed(y_test)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5001441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (5671, 1, 59, 7)\n",
      "Input shape for the model: (1, 59, 7)\n",
      "Model built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'lncrna_dataset' is already loaded and preprocessed\n",
    "sequence_data = np.array(lncrna_dataset['sequence_windows'].tolist())  # Example path\n",
    "structure_data = np.array(lncrna_dataset['structure_windows'].tolist())  # Example path\n",
    "X = np.concatenate([sequence_data, structure_data], axis=-1)\n",
    "\n",
    "# Check input data shape\n",
    "print(\"Combined data shape:\", X.shape)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = (1, 59, 7)  # As per your data\n",
    "print(\"Input shape for the model:\", input_shape)\n",
    "\n",
    "# Build the model\n",
    "try:\n",
    "    model = build_transformer_model(input_shape, num_layers=4, model_size=64, num_heads=8, ff_dim=256, dropout=0.1)\n",
    "    print(\"Model built successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Model building failed:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2bef6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5671, 1, 59, 7)\n",
      "Shape of y: (424,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of samples between features and labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_178648/2821935481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Ensure the number of samples match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mismatch in the number of samples between features and labels.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Correcting a hypothetical error (for illustration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of samples between features and labels."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Simulated data loading function\n",
    "def load_data():\n",
    "    # Simulating an issue where 'X' has more entries than 'y'\n",
    "    X = np.random.rand(5671, 1, 59, 7)  # For example, the combined data shape you mentioned\n",
    "    y = np.random.randint(2, size=(424,))  # Incorrect number of labels\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data()\n",
    "\n",
    "# Checking shapes\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Ensure the number of samples match\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(\"Mismatch in the number of samples between features and labels.\")\n",
    "\n",
    "# Correcting a hypothetical error (for illustration)\n",
    "# Let's assume the correct labels are not loaded properly\n",
    "y = np.random.randint(2, size=(X.shape[0],))  # Correcting the shape by creating a new 'y'\n",
    "\n",
    "# Now split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Continue with model training, etc.\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Output the training history to check for accuracy and loss improvements\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7268852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geneId</th>\n",
       "      <th>geneName</th>\n",
       "      <th>mirna</th>\n",
       "      <th>species</th>\n",
       "      <th>cell_line</th>\n",
       "      <th>tissue</th>\n",
       "      <th>category</th>\n",
       "      <th>method</th>\n",
       "      <th>positive_negative</th>\n",
       "      <th>direct_indirect</th>\n",
       "      <th>...</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Sequence_structure</th>\n",
       "      <th>encoded_sequence</th>\n",
       "      <th>encoded_structure</th>\n",
       "      <th>padded_sequences</th>\n",
       "      <th>padded_structures</th>\n",
       "      <th>sequence_windows</th>\n",
       "      <th>structure_windows</th>\n",
       "      <th>integrated_data</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000002079</td>\n",
       "      <td>MYH16</td>\n",
       "      <td>hsa-miR-4786-3p</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>293S</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>Embryonic/Fetal</td>\n",
       "      <td>HITS-CLIP</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>...</td>\n",
       "      <td>CTGAACAGCCAGCCCAGTATGCCAAGGTCTTCTCTGCTTGGACTTA...</td>\n",
       "      <td>........[[.(([[[........[[[[[[[[[[[(([[[[((((....</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...</td>\n",
       "      <td>[[[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], ...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0,...</td>\n",
       "      <td>[[[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000067601</td>\n",
       "      <td>PMS2P4</td>\n",
       "      <td>hsa-miR-24-3p</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brain</td>\n",
       "      <td>Normal/Primary</td>\n",
       "      <td>HITS-CLIP</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>...</td>\n",
       "      <td>TACAGAACCTGCTAAGGCCATCAAACCTATCGATCGGAAGTCAGTC...</td>\n",
       "      <td>.......(((....)))..........................[[....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], ...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0,...</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000073905</td>\n",
       "      <td>VDAC1P1</td>\n",
       "      <td>hsa-miR-1179</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>293S</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>Embryonic/Fetal</td>\n",
       "      <td>HITS-CLIP</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>...</td>\n",
       "      <td>ATGGCTGTGCCACCTACGTATGCTGATCTTGGCAAATCTGCCAGGG...</td>\n",
       "      <td>..((((((((.............((..(([[[((....))[[[))....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], ...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0,...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000078319</td>\n",
       "      <td>PMS2P1</td>\n",
       "      <td>hsa-miR-130a-3p</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>MCF7</td>\n",
       "      <td>Mammary Gland</td>\n",
       "      <td>Cancer/Malignant</td>\n",
       "      <td>HITS-CLIP</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>...</td>\n",
       "      <td>TTGGAGCGAGCTGAGAGCTCGAGTACAGAACCTGCTAAGGCCATCA...</td>\n",
       "      <td>...(((.((((..((((([[.......................((....</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], ...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0,...</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000080947</td>\n",
       "      <td>CROCCP3</td>\n",
       "      <td>hsa-miR-1224-3p</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Beta Cells</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>Normal/Primary</td>\n",
       "      <td>HITS-CLIP</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>DIRECT</td>\n",
       "      <td>...</td>\n",
       "      <td>CAGAGCCTGGAGTTGCAGAGGCAGCTACAGGAGGAGCAGGCCTCCT...</td>\n",
       "      <td>..((.((((.((((((....)))))).))))...........[[[....</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...</td>\n",
       "      <td>[[[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], ...</td>\n",
       "      <td>[[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0,...</td>\n",
       "      <td>[[[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            geneId geneName            mirna       species   cell_line  \\\n",
       "0  ENSG00000002079    MYH16  hsa-miR-4786-3p  Homo sapiens        293S   \n",
       "1  ENSG00000067601   PMS2P4    hsa-miR-24-3p  Homo sapiens         NaN   \n",
       "2  ENSG00000073905  VDAC1P1     hsa-miR-1179  Homo sapiens        293S   \n",
       "3  ENSG00000078319   PMS2P1  hsa-miR-130a-3p  Homo sapiens        MCF7   \n",
       "4  ENSG00000080947  CROCCP3  hsa-miR-1224-3p  Homo sapiens  Beta Cells   \n",
       "\n",
       "          tissue          category     method positive_negative  \\\n",
       "0         Kidney   Embryonic/Fetal  HITS-CLIP          POSITIVE   \n",
       "1          Brain    Normal/Primary  HITS-CLIP          POSITIVE   \n",
       "2         Kidney   Embryonic/Fetal  HITS-CLIP          POSITIVE   \n",
       "3  Mammary Gland  Cancer/Malignant  HITS-CLIP          POSITIVE   \n",
       "4       Pancreas    Normal/Primary  HITS-CLIP          POSITIVE   \n",
       "\n",
       "  direct_indirect  ...                                           Sequence  \\\n",
       "0          DIRECT  ...  CTGAACAGCCAGCCCAGTATGCCAAGGTCTTCTCTGCTTGGACTTA...   \n",
       "1          DIRECT  ...  TACAGAACCTGCTAAGGCCATCAAACCTATCGATCGGAAGTCAGTC...   \n",
       "2          DIRECT  ...  ATGGCTGTGCCACCTACGTATGCTGATCTTGGCAAATCTGCCAGGG...   \n",
       "3          DIRECT  ...  TTGGAGCGAGCTGAGAGCTCGAGTACAGAACCTGCTAAGGCCATCA...   \n",
       "4          DIRECT  ...  CAGAGCCTGGAGTTGCAGAGGCAGCTACAGGAGGAGCAGGCCTCCT...   \n",
       "\n",
       "                                  Sequence_structure  \\\n",
       "0  ........[[.(([[[........[[[[[[[[[[[(([[[[((((....   \n",
       "1  .......(((....)))..........................[[....   \n",
       "2  ..((((((((.............((..(([[[((....))[[[))....   \n",
       "3  ...(((.((((..((((([[.......................((....   \n",
       "4  ..((.((((.((((((....)))))).))))...........[[[....   \n",
       "\n",
       "                                    encoded_sequence  \\\n",
       "0  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...   \n",
       "1  [[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [...   \n",
       "2  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...   \n",
       "3  [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [...   \n",
       "4  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...   \n",
       "\n",
       "                                   encoded_structure  \\\n",
       "0  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...   \n",
       "1  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...   \n",
       "2  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...   \n",
       "3  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...   \n",
       "4  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...   \n",
       "\n",
       "                                    padded_sequences  \\\n",
       "0  [[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...   \n",
       "1  [[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], [...   \n",
       "2  [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [...   \n",
       "3  [[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [...   \n",
       "4  [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...   \n",
       "\n",
       "                                   padded_structures  \\\n",
       "0  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...   \n",
       "1  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...   \n",
       "2  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...   \n",
       "3  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, ...   \n",
       "4  [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, ...   \n",
       "\n",
       "                                    sequence_windows  \\\n",
       "0  [[[0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], ...   \n",
       "1  [[[0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0], ...   \n",
       "2  [[[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], ...   \n",
       "3  [[[0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], ...   \n",
       "4  [[[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], ...   \n",
       "\n",
       "                                   structure_windows  \\\n",
       "0  [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0,...   \n",
       "1  [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0,...   \n",
       "2  [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0,...   \n",
       "3  [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0,...   \n",
       "4  [[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0,...   \n",
       "\n",
       "                                     integrated_data labels  \n",
       "0  [[[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0...      0  \n",
       "1  [[[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 0...      0  \n",
       "2  [[[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0...      0  \n",
       "3  [[[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0...      0  \n",
       "4  [[[0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0...      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8dacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d27b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110f563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900c973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e285ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696fd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282f2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864bf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b132de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be4458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51e2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0f2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4f1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af21505-e8eb-4a7f-bc56-fa9cd015cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e947616-04ae-49a2-8cae-a4d8cd56acb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
