{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f7df1",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing for RNA Sequences\n",
    "\n",
    "### Description:\n",
    "### Load Data Function (load_data):\n",
    "- This function takes a file path and a column name as inputs. It reads the data from the specified CSV file into a dataframe (a table-like structure in Python).\n",
    "- It checks if there are any missing values in the specified column that contains RNA sequences. If missing values are found, it removes those rows from the dataframe to ensure that the analysis is done only on complete data.\n",
    "### One-Hot Encoding Function (one_hot_encode):\n",
    "- This function takes an RNA sequence as input. RNA sequences are made up of nucleotides represented by the letters A, C, G, and U.\n",
    "- It checks if the input sequence is in the correct format (i.e., it's a string). If itâ€™s not, the function returns None, which is a way of indicating missing or incorrect data.\n",
    "- The sequence is then converted to uppercase and any occurrence of 'T' (thymine, which is found in DNA but not RNA) is replaced with 'U' (uracil, which is specific to RNA).\n",
    "- The function then converts the sequence into a one-hot encoded format. This means each nucleotide (A, C, G, U) is represented as a vector (a list of numbers). For example, \n",
    "A is represented as [1, 0, 0, 0], C as [0, 1, 0, 0], G as [0, 0, 1, 0], and U as [0, 0, 0, 1]. This numerical representation is useful for computational models that require numeric input.\n",
    "### Loading Specific Datasets (ENCORI and LncBase):\n",
    "- The code then loads two datasets from CSV files: one called ENCORI_miRNA_lncRNA.csv and another called lncbase_with_sequences.csv. These datasets contain RNA sequences along with other biological information.\n",
    "It applies the one-hot encoding function to specific columns in these datasets that contain RNA sequences. This prepares the data for further analysis or modeling, where understanding the relationships between sequences can be crucial, such as predicting RNA interactions or functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c214b7e-a987-46ff-ab0d-916028376521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_encode_data(file_path, sequence_column, structure_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(subset=[sequence_column, structure_column], inplace=True)\n",
    "    df['encoded_sequence'] = df[sequence_column].apply(one_hot_encode)\n",
    "    df['encoded_structure'] = df[structure_column].apply(encode_structure)\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'U': [0, 0, 0, 1]}\n",
    "    return np.array([mapping.get(nucleotide, [0, 0, 0, 0]) for nucleotide in sequence.upper().replace('T', 'U')], dtype=np.float32)\n",
    "\n",
    "def encode_structure(structure):\n",
    "    mapping = {'.': [1, 0, 0], '(': [0, 1, 0], ')': [0, 0, 1]}\n",
    "    return np.array([mapping.get(char, [0, 0, 0]) for char in structure], dtype=np.float32)\n",
    "\n",
    "def pad_encoded_data(encoded_data, max_length):\n",
    "    padded = np.zeros((len(encoded_data), max_length, len(encoded_data[0][0])), dtype=np.float32)\n",
    "    for i, seq in enumerate(encoded_data):\n",
    "        length = min(len(seq), max_length)\n",
    "        padded[i, :length, :] = seq[:length]\n",
    "    return padded\n",
    "\n",
    "def create_overlapping_windows(data, window_size, overlap):\n",
    "    step = window_size - overlap\n",
    "    num_windows = (len(data) - window_size) // step + 1\n",
    "    windows = [data[i * step: i * step + window_size] for i in range(num_windows) if i * step + window_size <= len(data)]\n",
    "    return np.array(windows)\n",
    "\n",
    "\n",
    "def prepare_dataset(file_path, sequence_column, structure_column, window_size=500, overlap=250, label_column=None, threshold=None):\n",
    "    df = load_and_encode_data(file_path, sequence_column, structure_column)\n",
    "    \n",
    "    # Adjust max_length to be dynamic or based on the longest sequence\n",
    "    max_length = max(df['encoded_sequence'].apply(len).max(), df['encoded_structure'].apply(len).max())\n",
    "    \n",
    "    df['padded_sequences'] = df['encoded_sequence'].apply(lambda x: pad_encoded_data([x], max_length)[0])\n",
    "    df['padded_structures'] = df['encoded_structure'].apply(lambda x: pad_encoded_data([x], max_length)[0])\n",
    "    \n",
    "    # Ensure windowing covers the entire sequence dynamically\n",
    "    df['sequence_windows'] = df['padded_sequences'].apply(lambda x: create_overlapping_windows(x, window_size, overlap))\n",
    "    df['structure_windows'] = df['padded_structures'].apply(lambda x: create_overlapping_windows(x, window_size, overlap))\n",
    "    \n",
    "    df['integrated_data'] = df.apply(lambda row: np.concatenate((row['sequence_windows'], row['structure_windows']), axis=-1), axis=1)\n",
    "\n",
    "    if label_column:\n",
    "        if threshold is not None:\n",
    "            df['labels'] = df[label_column].apply(lambda x: 1 if x >= threshold else 0)\n",
    "        else:\n",
    "            df['labels'] = df[label_column].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5094c137-fcfb-4c2b-929f-803e17baebfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mirna_dataset = prepare_dataset(\n",
    "    'dataset/mirna_sequences.csv', 'miRseq', 'miRseq_structure', \n",
    "    label_column='clipExpNum', threshold=10\n",
    ")\n",
    "\n",
    "lncrna_dataset = prepare_dataset(\n",
    "    'dataset/lncbase_with_sequences.csv', 'Sequence', 'Sequence_structure', \n",
    "    label_column='positive_negative'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncrna_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4c3ba",
   "metadata": {},
   "source": [
    "## Mirna Dataset Structrue and Sequence combined transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16078b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_positional_encoding(seq_length, model_size):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(model_size) // 2)) / np.float32(model_size))\n",
    "    angle_rads = np.arange(seq_length)[:, np.newaxis] * angle_rates\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = np.expand_dims(pos_encoding, 0)  # Add batch dimension for broadcasting\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(inputs, model_size, num_heads, ff_dim, dropout_rate):\n",
    "    seq_length = inputs.shape[1]  # Ensure this is correct\n",
    "    model_size = inputs.shape[2]  # Ensure this matches the last dimension of inputs\n",
    "    pos_encoding = get_positional_encoding(seq_length, model_size)\n",
    "    inputs += pos_encoding  # Add positional encoding to inputs\n",
    "\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_size, dropout=dropout_rate)\n",
    "    attention_output = attention_layer(inputs, inputs)\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = tf.keras.layers.Add()([inputs, attention_output])\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "\n",
    "    ff_layer_one = tf.keras.layers.Dense(ff_dim, activation='relu')\n",
    "    ff_layer_two = tf.keras.layers.Dense(model_size)\n",
    "    ff_output = ff_layer_one(attention_output)\n",
    "    ff_output = tf.keras.layers.Dropout(dropout_rate)(ff_output)\n",
    "    ff_output = ff_layer_two(ff_output)\n",
    "    ff_output = tf.keras.layers.Add()([attention_output, ff_output])\n",
    "    ff_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ff_output)\n",
    "\n",
    "    return ff_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fc2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_sequence_only_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Applying multiple layers of the transformer encoder\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # Removing any singleton dimensions and applying pooling\n",
    "    if x.shape[1] == 1:\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "X = np.array(mirna_dataset['sequence_windows'].tolist())\n",
    "y = np.array(mirna_dataset['labels'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulating the loading and processing of sequence data\n",
    "def simulate_data_processing():\n",
    "    # Simulate some sequence data\n",
    "    sequence_data = np.random.randint(0, 4, (424, 100))  # Example: 424 sequences of length 100\n",
    "    # One-hot encode the sequence data\n",
    "    sequence_encoded = np.eye(4)[sequence_data]  # Example: one-hot encoding\n",
    "    return sequence_encoded.reshape(424, -1, 4)  # Reshape for model input: (batch, sequence_length, features)\n",
    "\n",
    "# Load and process data\n",
    "X = simulate_data_processing()\n",
    "y = np.random.randint(0, 2, 424)  # Random binary labels\n",
    "\n",
    "print(\"Shape of X after processing:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Define model parameters\n",
    "input_shape = X.shape[1:]  # Dynamic input shape based on data\n",
    "num_layers = 4\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "ff_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Build the model, ensuring the input shape is correctly passed\n",
    "try:\n",
    "    sequence_model = build_sequence_only_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout)\n",
    "except Exception as e:\n",
    "    print(\"Error in building the model:\", e)\n",
    "    raise\n",
    "\n",
    "# Split data for training\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "except Exception as e:\n",
    "    print(\"Error during train-test split:\", e)\n",
    "    raise\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = sequence_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "except Exception as e:\n",
    "    print(\"Error during model training:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_positional_encoding(seq_length, model_size):\n",
    "    if seq_length == 0:\n",
    "        raise ValueError(\"Sequence length is zero, which is not valid for positional encoding.\")\n",
    "    angle_rates = 1 / np.power(10000, (2 * (np.arange(model_size) // 2)) / np.float32(model_size))\n",
    "    angle_rads = np.arange(seq_length)[:, np.newaxis] * angle_rates\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = np.expand_dims(pos_encoding, 0)  # Add batch dimension for broadcasting\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def transformer_encoder(inputs, num_heads, ff_dim, dropout_rate, model_size):\n",
    "    if inputs.shape[1] == 0:\n",
    "        raise ValueError(\"Input sequence length is zero, check your data preprocessing.\")\n",
    "    seq_length = inputs.shape[1]\n",
    "    model_size = inputs.shape[2]\n",
    "    pos_encoding = get_positional_encoding(seq_length, model_size)\n",
    "    inputs += pos_encoding\n",
    "\n",
    "    attention_layer = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_size, dropout=dropout_rate)\n",
    "    attention_output = attention_layer(inputs, inputs)\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = tf.keras.layers.Add()([inputs, attention_output])\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "\n",
    "    ff_layer_one = tf.keras.layers.Dense(ff_dim, activation='relu')\n",
    "    ff_layer_two = tf.keras.layers.Dense(model_size)\n",
    "    ff_output = ff_layer_one(attention_output)\n",
    "    ff_output = tf.keras.layers.Dropout(dropout_rate)(ff_output)\n",
    "    ff_output = ff_layer_two(ff_output)\n",
    "    ff_output = tf.keras.layers.Add()([attention_output, ff_output])\n",
    "    ff_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ff_output)\n",
    "\n",
    "    return ff_output\n",
    "\n",
    "def build_combined_model(input_shape, num_layers, head_size, num_heads, ff_dim, dropout):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, num_heads, ff_dim, dropout, head_size)\n",
    "\n",
    "    if x.shape[1] == 1:\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e48522",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Assuming the data loading and preprocessing have been done correctly\n",
    "    sequence_data = np.array(mirna_dataset['sequence_windows'].tolist())\n",
    "    structure_data = np.array(mirna_dataset['structure_windows'].tolist())\n",
    "    X = np.concatenate([sequence_data, structure_data], axis=-1)  # Concatenate along the last dimension\n",
    "    y = np.array(mirna_dataset['labels'].tolist())\n",
    "\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(\"Sequence length is zero after concatenation, check your data.\")\n",
    "\n",
    "    input_shape = X.shape[1:]  # (window_size, num_features)\n",
    "    model = build_combined_model(input_shape, num_layers=4, head_size=64, num_heads=4, ff_dim=256, dropout=0.1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "    print(\"Training complete.\")\n",
    "except Exception as e:\n",
    "    print(\"Error encountered:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57608ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c080fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
